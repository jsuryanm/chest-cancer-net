# End-to-End Chest Cancer Classification 

An **End-to-End Deep Learning system** for **Chest CT Scan Cancer Classification**, built with **PyTorch**, **FastAPI**, **Streamlit**, **Docker**, **DVC**, and **MLflow**.

This repository follows **MLOps practices** using modular pipelines, reproducibility, experiment tracking, containerized deployment, and cloud-native CI/CD for efficient implementation.

---

## Problem Statement

Chest cancer is one of the leading causes of cancer-related deaths.  
Early detection using **CT scan imaging** enables faster diagnosis and better patient outcomes.

This system classifies chest CT scans into:
- **Normal**
- **Cancer (Adenocarcinoma)**

---

---

## Why ResNet-18?

- Lightweight
- Fast inference
- Strong transfer learning
- Ideal for medical datasets

---

## Model Overview

| Component | Description |
|---------|-------------|
| Architecture | ResNet-18 (Transfer Learning) |
| Framework | PyTorch |
| Task | Binary Image Classification |
| Loss | BCEWithLogitsLoss |
| Optimizer | AdamW |
| Metrics | Accuracy, Precision, Recall, F1 |
| Tracking | MLflow (DAGsHub) |

---

## Project Directory Structure (Authoritative)

```
CHEST-CANCER-NET/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ deploy.yml
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”œâ”€â”€ hyperparameter_tuning/
â”‚   â”œâ”€â”€ prepare_base_model/
â”‚   â””â”€â”€ training/
â”‚       â””â”€â”€ model_best_hparams.pt
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_best_hparams.pt
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ logs/
â”œâ”€â”€ research/
â”œâ”€â”€ src/cancer_clf/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ entity/
â”‚   â”œâ”€â”€ logger/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ app.py
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ Dockerfile.api
â”œâ”€â”€ Dockerfile.streamlit
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ params.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## End-to-End Pipeline

1. Data Ingestion  
2. Prepare Base Model  
3. Hyperparameter Tuning (Optuna)  
4. Model Training  
5. Model Evaluation (MLflow)  
6. Inference (FastAPI + Streamlit)  

---

## Local Development

```bash
git clone https://github.com/<your-username>/chest-cancer-net.git
cd chest-cancer-net

conda create -n chest-cancer python=3.10 -y
conda activate chest-cancer
pip install -r requirements.txt
```

Run full pipeline:
```bash
dvc repro
```

---

## Docker (Local)

```bash
docker-compose up --build
```

---

# AWS CONFIGURATION 

This setup supports **CI/CD using GitHub Actions â†’ Amazon ECR â†’ EC2**,  
âœ… No secrets stored on EC2  
âœ… No retraining during deployment  

---

## ğŸ§± Architecture

```
GitHub Actions (CI)
 â”œâ”€â”€ Build Docker images
 â”œâ”€â”€ Push images to Amazon ECR
 â†“
EC2 (CD)
 â”œâ”€â”€ Pull images from ECR (IAM Role)
 â”œâ”€â”€ Run containers using Docker Compose
```

---

## PART 1ï¸âƒ£ â€” AWS Prerequisites

- AWS account
- Billing enabled
- Region selected (example: ap-southeast-1)

---

## PART 2ï¸âƒ£ â€” Create ECR Repositories

Create **two private repositories**:

### Repository 1 (API)
- Name: `chest-cancer-api`

### Repository 2 (UI)
- Name: `chest-cancer-ui`

Note:
- AWS Account ID
- AWS Region

---

## PART 3ï¸âƒ£ â€” IAM User (GitHub Actions CI)

### Create User
- Name: `github-actions-ci`
- Access type: Programmatic

### Attach Policy
- `AmazonEC2ContainerRegistryFullAccess`

### Save Credentials
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`

---

## PART 4ï¸âƒ£ â€” IAM Role (EC2 CD)

### Create Role
- Trusted entity: EC2

### Attach Policy
- `AmazonEC2ContainerRegistryReadOnly`

Role name:
```
EC2-ECR-READONLY
```

---

## PART 5ï¸âƒ£ â€” Create EC2 Instance

- AMI: Ubuntu 22.04
- Instance type: t2.micro / t3.micro
- Key pair: `infer.pem`
- IAM Role: `EC2-ECR-READONLY`

### Security Group (IMPORTANT)

| Type | Port | Source |
|----|----|----|
| SSH | 22 | 0.0.0.0/0 |
| Custom TCP | 8000 | 0.0.0.0/0 |
| Custom TCP | 8501 | 0.0.0.0/0 |

---

## PART 6ï¸âƒ£ â€” EC2 Setup (One-Time)

```bash
ssh -i infer.pem ubuntu@<EC2_PUBLIC_IP>

sudo apt update -y
sudo apt upgrade -y

curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

sudo apt install -y docker-compose-plugin awscli
sudo usermod -aG docker ubuntu
newgrp docker
```

Verify:
```bash
docker --version
docker compose version
aws --version
aws sts get-caller-identity
```

âš ï¸ Do NOT run `aws configure`  
âš ï¸ Do NOT add AWS keys on EC2  

---

## PART 7ï¸âƒ£ â€” GitHub Secrets

Add in **GitHub â†’ Settings â†’ Secrets â†’ Actions**

### AWS (CI)
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_REGION`
- `AWS_ACCOUNT_ID`

### EC2 (CD)
- `EC2_HOST`
- `EC2_USERNAME` â†’ ubuntu
- `EC2_SSH_KEY` â†’ contents of `infer.pem`

---

## PART 8ï¸âƒ£ â€” CI/CD Flow

On push to `main`:

**CI**
- Build API & UI Docker images
- Push images to ECR

**CD**
- SSH into EC2
- Pull latest images
- Generate docker-compose.yml
- Restart containers

---

## PART 9ï¸âƒ£ â€” Verify Deployment

- API: `http://<EC2_PUBLIC_IP>:8000/docs`
- Streamlit: `http://<EC2_PUBLIC_IP>:8501`




---

## PART ğŸ”Ÿ â€” Challenges Faced & Learnings

Building an end-to-end, production-ready MLOps system involves addressing real-world engineering challenges. Below are the key challenges encountered during development and deployment, along with the solutions and learnings.

---

### 1ï¸âƒ£ Docker Build Context & `.dockerignore` Issues

**Challenge:**  
Docker builds failed due to ignored model files or missing build context.

**Solution:**  
- Carefully configured `.dockerignore`
- Explicitly allowed inference models while excluding large training artifacts
- Added fail-fast validation inside Dockerfiles

**Learning:**  
> Docker context management is critical for ML workloads due to large artifacts.

---

### 2ï¸âƒ£ Secure AWS Authentication Without Secrets on EC2

**Challenge:**  
AWS CLI failed on EC2 due to missing credentials during deployment.

**Solution:**  
- Used **IAM User** for GitHub Actions (CI)
- Used **IAM Role** for EC2 (CD)
- Eliminated AWS access keys on EC2 entirely

**Learning:**  
> IAM roles are the safest and recommended way to authenticate AWS services running on EC2.

---

### 3ï¸âƒ£ Docker Compose Deployment Failures

**Challenge:**  
`docker compose` failed because `docker-compose.yml` was not present on EC2.

**Solution:**  
- CI/CD pipeline dynamically generates `docker-compose.yml` during deployment
- Removed dependency on manual EC2 setup or repository cloning

**Learning:**  
> Deployment pipelines should be **idempotent** and should not rely on server state.

---


## PART 1ï¸âƒ£1ï¸âƒ£ â€” Future Improvements

While this system is production-ready, the following improvements can further enhance scalability, security, and observability.

---

### ğŸš€ Infrastructure Improvements
- Migrate deployment from EC2 to **Amazon ECS or EKS**
- Add **Auto Scaling Groups** for high availability
- Introduce **Application Load Balancer (ALB)**

---

### ğŸ” Security Enhancements
- Enable **HTTPS** using Nginx + Letâ€™s Encrypt
- Restrict SSH access using IP whitelisting
- Apply stricter IAM least-privilege policies

---

### ğŸ“ˆ Monitoring & Observability
- Integrate **AWS CloudWatch** for logs and metrics
- Add application health checks
- Track inference latency and throughput


### ğŸ§  ML Improvements
- Extend to multi-class classification (additional cancer types)
- Add MLflow Model Registry integration
- Implement shadow deployment for new models
- Add data drift detection


â­ Star this repository if it helped you!
